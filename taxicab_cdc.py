import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from datetime import timedelta
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import rank, first, last,max
from awsglue.dynamicframe import DynamicFrame
import pyspark.sql.functions as sf


glueContext = GlueContext(SparkContext.getOrCreate())

initialDyF = glueContext.create_dynamic_frame.from_options(
        's3',
        {'paths': ['s3://pheaa-initial-load/']},
        'csv',
        {'withHeader': True, 'separator': '|'})

cdcDyF = glueContext.create_dynamic_frame.from_options(
        's3',
        {'paths': ['s3://pheaa-cdc-load/']},
        'csv',
        {'withHeader': True, 'separator': '|'})

initialDf=initialDyF.toDF()

cdcDf=cdcDyF.toDF()

initialDf = initialDf.\
withColumn("year",sf.year("tpep_pickup_datetime")).\
withColumn("month",sf.month("tpep_pickup_datetime")).\
withColumn("day",sf.dayofmonth("tpep_pickup_datetime"))

initialDyf = DynamicFrame.fromDF(initialDf, glueContext, "initialDynamicFrame")

glueContext.write_dynamic_frame.from_options(
    frame = initialDyf,
    connection_type = "s3",    
    connection_options = {"path": "s3://pheaa-base-partitioned/partitionedtable/","partitionKeys": ["year","month","day"]},
    format = "parquet")

window = Window.\
              partitionBy(cdcDf.pk1).\
              orderBy(cdcDf.transaction_time).\
              rangeBetween(-sys.maxsize, sys.maxsize)

window2 = Window.\
              partitionBy(cdcDf.pk1)

temp1Df= cdcDf.\
withColumn("f_val",first(cdcDf.op_cd).over(window)).\
withColumn("l_val",last(cdcDf.op_cd).over(window)).\
withColumn("max_dt",max(cdcDf.transaction_time).over(window2))

udDf = temp1Df.filter("transaction_time= max_dt").filter("op_cd IN ('U','D')").\
withColumnRenamed("op_cd",'b_op_cd').\
withColumnRenamed("pk1",'b_pk1').\
withColumnRenamed("vendorid",'b_vendorid').\
withColumnRenamed("tpep_pickup_datetime",'b_tpep_pickup_datetime').\
withColumnRenamed("tpep_dropoff_datetime",'b_tpep_dropoff_datetime').\
withColumnRenamed("passenger_count",'b_passenger_count').\
withColumnRenamed("trip_distance",'b_trip_distance').\
withColumnRenamed("ratecodeid",'b_ratecodeid').\
withColumnRenamed("store_and_fwd_flag",'b_store_and_fwd_flag').\
withColumnRenamed("pulocationid",'b_pulocationid').\
withColumnRenamed("dolocationid",'b_dolocationid').\
withColumnRenamed("payment_type",'b_payment_type').\
withColumnRenamed("fare_amount",'b_fare_amount').\
withColumnRenamed("extra",'b_extra').\
withColumnRenamed("mta_tax",'b_mta_tax').\
withColumnRenamed("tip_amount",'b_tip_amount').\
withColumnRenamed("tolls_amount",'b_tolls_amount').\
withColumnRenamed("improvement_surcharge",'b_improvement_surcharge').\
withColumnRenamed("total_amount",'b_total_amount').\
withColumnRenamed("transaction_time",'b_transaction_time')

baseDf = spark.read.parquet('s3://pheaa-base-partitioned/partitionedtable/')

yearList=[p[0] for p in udDf.select(sf.year(udDf.b_tpep_pickup_datetime)).distinct().collect()]

monthList=[p[0] for p in udDf.select(sf.month(udDf.b_tpep_pickup_datetime)).distinct().collect()]

dayList=[p[0] for p in udDf.select(sf.dayofmonth(udDf.b_tpep_pickup_datetime)).distinct().collect()]

fiteredBaseDf = baseDf.\
filter(baseDf.year.isin(yearList)).\
filter(baseDf.month.isin(monthList)).\
filter(baseDf.day.isin(dayList))

cond = [fiteredBaseDf.pk1==udDf.b_pk1]

joinedUdDf = fiteredBaseDf.\
join(udDf,cond, 'left')

finalUdDf = joinedUdDf.selectExpr("b_op_cd",
"CASE WHEN b_op_cd	='D' OR b_pk1	IS NULL THEN	pk1	ELSE	b_pk1	END	pk1",
"CASE WHEN b_op_cd	='D' OR b_vendorid	IS NULL THEN	vendorid	ELSE	b_vendorid	END	vendorid",
"CASE WHEN b_op_cd	='D' OR b_tpep_pickup_datetime	IS NULL THEN	tpep_pickup_datetime	ELSE	b_tpep_pickup_datetime	END	tpep_pickup_datetime",
"CASE WHEN b_op_cd	='D' OR b_tpep_dropoff_datetime	IS NULL THEN	tpep_dropoff_datetime	ELSE	b_tpep_dropoff_datetime	END	tpep_dropoff_datetime",
"CASE WHEN b_op_cd	='D' OR b_passenger_count	IS NULL THEN	passenger_count	ELSE	b_passenger_count	END	passenger_count",
"CASE WHEN b_op_cd	='D' OR b_trip_distance	IS NULL THEN	trip_distance	ELSE	b_trip_distance	END	trip_distance",
"CASE WHEN b_op_cd	='D' OR b_ratecodeid	IS NULL THEN	ratecodeid	ELSE	b_ratecodeid	END	ratecodeid",
"CASE WHEN b_op_cd	='D' OR b_store_and_fwd_flag	IS NULL THEN	store_and_fwd_flag	ELSE	b_store_and_fwd_flag	END	store_and_fwd_flag",
"CASE WHEN b_op_cd	='D' OR b_pulocationid	IS NULL THEN	pulocationid	ELSE	b_pulocationid	END	pulocationid",
"CASE WHEN b_op_cd	='D' OR b_dolocationid	IS NULL THEN	dolocationid	ELSE	b_dolocationid	END	dolocationid",
"CASE WHEN b_op_cd	='D' OR b_payment_type	IS NULL THEN	payment_type	ELSE	b_payment_type	END	payment_type",
"CASE WHEN b_op_cd	='D' OR b_fare_amount	IS NULL THEN	fare_amount	ELSE	b_fare_amount	END	fare_amount",
"CASE WHEN b_op_cd	='D' OR b_extra	IS NULL THEN	extra	ELSE	b_extra	END	extra",
"CASE WHEN b_op_cd	='D' OR b_mta_tax	IS NULL THEN	mta_tax	ELSE	b_mta_tax	END	mta_tax",
"CASE WHEN b_op_cd	='D' OR b_tip_amount	IS NULL THEN	tip_amount	ELSE	b_tip_amount	END	tip_amount",
"CASE WHEN b_op_cd	='D' OR b_tolls_amount	IS NULL THEN	tolls_amount	ELSE	b_tolls_amount	END	tolls_amount",
"CASE WHEN b_op_cd	='D' OR b_improvement_surcharge	IS NULL THEN	improvement_surcharge	ELSE	b_improvement_surcharge	END	improvement_surcharge",
"CASE WHEN b_op_cd	='D' OR b_total_amount	IS NULL THEN	total_amount	ELSE	b_total_amount	END	total_amount",
"CASE WHEN b_op_cd	='D' OR b_transaction_time	IS NULL THEN	transaction_time	ELSE	b_transaction_time	END	transaction_time"
                                 )                    

iDf = temp1Df.filter("transaction_time= max_dt").filter("f_val ='I'").\
drop('op_cd').\
withColumnRenamed("l_val",'op_cd')

finalIDf=iDf.select("pk1","vendorid","tpep_pickup_datetime","tpep_dropoff_datetime","passenger_count",
                    "trip_distance","ratecodeid","store_and_fwd_flag","pulocationid","dolocationid",
                    "payment_type","fare_amount","extra","mta_tax","tip_amount","tolls_amount",
                    "improvement_surcharge","total_amount","transaction_time")

finalUdDf = finalUdDf.drop('b_op_cd')


finalTable = finalUdDf.unionAll(finalIDf).\
withColumn("year",sf.year("tpep_pickup_datetime")).\
withColumn("month",sf.month("tpep_pickup_datetime")).\
withColumn("day",sf.dayofmonth("tpep_pickup_datetime"))

spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")

numPart = int(finalTable.count()/100000)

finalTable.coalesce(numPart).write.partitionBy(["year","month","day"]).\
mode("overwrite").parquet("s3://pheaa-base-partitioned/partitionedtable/")
